# backend/models/train_gru.py
"""
Supervised Pretraining Script for GRU Encoder
===============================================================

This script performs OFFLINE supervised pretraining of the GRU encoder
using historical market data.

train_gru.py receives:

✅ All engineered features (11 features per timestep)
✅ Trend labels (classification)
✅ Expected return labels (regression)

Nothing is missing, nothing is inferred implicitly.

Pipeline Overview
----------------------------------------------------------------
(1) CSV datasets (generated by generate_dataset.py)
        ↓
(2) GRUPretrainingDataset (datasets.py)
        ↓
(3) GRU Encoder + Multi-Head Supervised Model (gru_encoder.py)
        ↓
(4) Trained encoder weights + fitted scaler saved to disk

WHY THIS STAGE EXISTS
----------------------------------------------------------------
Reinforcement learning alone provides a weak learning signal in finance
(noisy rewards, delayed feedback, non-stationarity).

By pretraining the GRU encoder to predict:
    • trend direction
    • expected return magnitude

we ensure the hidden state already encodes:
    • momentum vs mean-reversion
    • strength of moves
    • temporal structure in indicators

The RL agent later receives *meaningful latent states*, not raw noise.

IMPORTANT UPDATE (VALIDATION)
----------------------------------------------------------------
We now explicitly split the dataset into:

    • TRAINING SET  → fits the scaler, used for optimization
    • VALIDATION SET → reuses scaler, used ONLY for monitoring generalization

This avoids data leakage and allows early detection of overfitting.


WHAT TO WATCH FOR WHILE IT IS RUNNING
----------------------------------------------------------------

RUN: $env:PYTHONPATH = "."; python -m backend.models.train_gru

What to watch for while it's running:

    The "Trend" Loss: This is your classification error. It usually starts around 1.09 (which is the natural log of 3 classes, representing a random guess). If it drops toward 0.80 - 0.95, your model is successfully learning market direction.

    The "Return" Loss: This is the regression error for your 3-day returns. It will likely be a very small number (e.g., 0.02).

    The "Val" (Validation) Column: This is the most important number.

        If Train loss goes down but Val loss starts going UP, your model is starting to overfit (memorizing the 2020-23 data instead of learning patterns that work in 2024).

        If Val is much lower than Train, you might have a very "easy" validation set (or a very small one).

THE FINAL SAVE
----------------------------------------------------------------

Once Epoch 25 finishes, the script will automatically:

    Save the Encoder Weights to backend/models/checkpoints/gru_encoder.pt.

    Save the Scaler (the math used to normalize your features) to scaler.pkl.

    Print a final "TRAINING COMPLETE" message.

"""

import os
import glob
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# Local imports
from backend.models.gru_encoder import GRUSupervisedModel
from backend.models.datasets import GRUPretrainingDataset
from backend.utils.preprocessing import SEQUENCE_LENGTH, N_FEATURES


# =====================================================================================
# CONFIGURATION
# =====================================================================================

DATA_DIR = "backend/data/processed"

# Temporal split configuration
VAL_START_DATE = "2024-01-01"  # All rows BEFORE this date → training; All rows ON / AFTER this date → validation

# Early stopping configuration
EARLY_STOPPING_PATIENCE = 6
EARLY_STOPPING_MIN_DELTA = 1e-4

# Training hyperparameters
BATCH_SIZE = 64
NUM_EPOCHS = 25
LEARNING_RATE = 1e-3

# Model hyperparameters
HIDDEN_SIZE = 64
NUM_LAYERS = 1
DROPOUT = 0.0

# Output paths
MODEL_SAVE_PATH = "backend/models/checkpoints/gru_encoder.pt"
SCALER_SAVE_PATH = "backend/models/checkpoints/scaler.pkl"

os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# =====================================================================================
# FEATURE CONFIGURATION
# =====================================================================================

FEATURE_COLUMNS = [
    'Open',
    'Close',
    'RSI',
    'MACD',
    'MACD_Signal',
    'ATR',
    'SMA_50',
    'OBV',
    'ROC_10',
    'SMA_Ratio',
    'RealizedVol_20'
]

# =====================================================================================
# MODEL-SUMMARY PRINTOUT - PER EPOCH (Helper-Function)
# =====================================================================================

"""
With HIDDEN_SIZE=64, NUM_LAYERS=1, and N_FEATURES=11, your summary will likely show around 15,000 to 20,000 parameters.

    The GRU Weights: This is where the bulk of the parameters are. They control how the 11 inputs are mixed into the 64-dimensional hidden memory.

    The "Heads" (Trend & Return): These are simple linear layers at the end that translate the 64-dimensional hidden state into a class (Trend) or a value (Return).

    By seeing the parameter count, you are looking at the "size" of the brain you are building. Since your training set has roughly 2,300 windows (from 2 stocks, 2020-2023), having ~15k parameters is a healthy ratio. It’s enough "brain power" to learn the patterns without being so large that it just memorizes the specific dates.
"""

def print_model_summary(model):
    """
    Prints the number of trainable parameters.
    Helps verify if the model capacity matches the dataset size.
    """
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    
    print("-" * 30)
    print(f"MODEL ARCHITECTURE SUMMARY")
    print("-" * 30)
    print(f"Trainable Parameters: {trainable_params:,}")
    print(f"Total Parameters:     {total_params:,}")
    print("-" * 30)
    print(model)  # This prints the layer structure
    print("-" * 30 + "\n")

# =====================================================================================
# TRAINING FUNCTION
# =====================================================================================

def train():
    """
    Main training routine.

    Steps:
        1. Collect CSV paths (ALL symbols)
        2. Build temporally-split datasets (inside datasets.py)
        3. Build DataLoaders
        4. Initialize GRU supervised model
        5. Train using multi-task loss
        6. Monitor validation loss for generalization
        7. Save encoder + scaler
    """

    print("\n====================================")
    print(" GRU SUPERVISED PRETRAINING STARTED ")
    print("====================================\n")

    # ------------------------------------------------------------------
    # 1. Collect CSVs
    # ------------------------------------------------------------------
    csv_paths = sorted(glob.glob(os.path.join(DATA_DIR, "*.csv")))
    if len(csv_paths) == 0:
        raise FileNotFoundError(f"No CSV files found in {DATA_DIR}")

    print(f"Loaded {len(csv_paths)} symbol CSVs")
    print(f"Temporal validation boundary: {VAL_START_DATE}\n")

    # ------------------------------------------------------------------
    # 2. Build datasets
    # ------------------------------------------------------------------
    # IMPORTANT:
    #   • Training dataset fits the scaler (using TRAINING period only)
    #   • Validation dataset MUST reuse the same scaler
    #     to avoid data leakage
    #   • All symbols appear in BOTH train and validation sets
    train_dataset = GRUPretrainingDataset(
        csv_paths=csv_paths,
        feature_columns=FEATURE_COLUMNS,
        sequence_length=SEQUENCE_LENGTH,
        split="train",
        val_start_date=VAL_START_DATE,
        fit_scaler=True
    )

    val_dataset = GRUPretrainingDataset(
        csv_paths=csv_paths,
        feature_columns=FEATURE_COLUMNS,
        sequence_length=SEQUENCE_LENGTH,
        split="val",
        val_start_date=VAL_START_DATE,
        scaler=train_dataset.scaler,
        fit_scaler=False
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        drop_last=False
    )

    print(f"Training samples:   {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")
    print(f"Using device: {DEVICE}\n")

    # ------------------------------------------------------------------
    # 3. Initialize Model
    # ------------------------------------------------------------------
    model = GRUSupervisedModel(
        input_size=N_FEATURES,
        hidden_size=HIDDEN_SIZE,
        num_layers=NUM_LAYERS,
        dropout=DROPOUT
    ).to(DEVICE)

    # Print the summary before starting the loop
    print_model_summary(model)

    # ------------------------------------------------------------------
    # 4. Loss Functions + Optimizer
    # ------------------------------------------------------------------
    # Trend classification: 3 classes → CrossEntropyLoss
    trend_loss_fn = nn.CrossEntropyLoss()

    # Expected return regression: noisy targets → Huber is robust
    return_loss_fn = nn.SmoothL1Loss()

    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=LEARNING_RATE
    )

    # ------------------------------------------------------------------
    # 5. Training + Validation Loop
    # ------------------------------------------------------------------

    # Early stopping state initialization
    best_val_loss = float("inf")
    best_epoch = 0
    epochs_without_improvement = 0

    best_encoder_state = None


    for epoch in range(1, NUM_EPOCHS + 1):
        # ------------------------------
        # Training phase
        # ------------------------------
        model.train()

        train_total_loss = 0.0
        train_trend_loss = 0.0
        train_return_loss = 0.0

        for X, y_trend, y_return in train_loader:
            X = X.to(DEVICE)
            y_trend = y_trend.to(DEVICE)
            y_return = y_return.to(DEVICE)

            optimizer.zero_grad()

            trend_logits, return_pred = model(X)

            loss_trend = trend_loss_fn(trend_logits, y_trend)
            loss_return = return_loss_fn(return_pred, y_return)
            loss = loss_trend + loss_return

            # 1. Backpropagation: Compute gradients
            loss.backward()

            # 2. CLIP GRADIENTS for numerical stability
            # This prevents the "COVID outliers" from destabilizing the model
            torch.nn.utils.clip_grad_norm_(
                model.parameters(),
                max_norm=1.0
            )

            # 3. Update weights
            optimizer.step()

            train_total_loss += loss.item()
            train_trend_loss += loss_trend.item()
            train_return_loss += loss_return.item()

        # ------------------------------
        # Validation phase (no gradients)
        # ------------------------------
        model.eval()

        val_total_loss = 0.0
        val_trend_loss = 0.0
        val_return_loss = 0.0

        with torch.no_grad():
            for X, y_trend, y_return in val_loader:
                X = X.to(DEVICE)
                y_trend = y_trend.to(DEVICE)
                y_return = y_return.to(DEVICE)

                trend_logits, return_pred = model(X)

                loss_trend = trend_loss_fn(trend_logits, y_trend)
                loss_return = return_loss_fn(return_pred, y_return)
                loss = loss_trend + loss_return

                val_total_loss += loss.item()
                val_trend_loss += loss_trend.item()
                val_return_loss += loss_return.item()

        # ------------------------------
        # Epoch summary
        # ------------------------------
        print(
            f"Epoch [{epoch:02d}/{NUM_EPOCHS}] | "
            f"Train: {train_total_loss/len(train_loader):.4f} | "
            f"Val: {val_total_loss/len(val_loader):.4f} | "
            f"Trend (T/V): {train_trend_loss/len(train_loader):.4f}/"
            f"{val_trend_loss/len(val_loader):.4f} | "
            f"Return (T/V): {train_return_loss/len(train_loader):.4f}/"
            f"{val_return_loss/len(val_loader):.4f}"
        )

        # ----------------------------------------------------------
        # Early stopping check (based on validation total loss)
        # ----------------------------------------------------------
        avg_val_loss = val_total_loss / len(val_loader)

        if avg_val_loss < best_val_loss - EARLY_STOPPING_MIN_DELTA:
            best_val_loss = avg_val_loss
            best_epoch = epoch
            epochs_without_improvement = 0

            # Save best encoder weights only
            best_encoder_state = {
                k: v.detach().cpu().clone()
                for k, v in model.encoder.state_dict().items()
            }

        else:
            epochs_without_improvement += 1

            if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:
                print(
                    f"\nEarly stopping triggered at epoch {epoch}. "
                    f"Best validation loss: {best_val_loss:.4f} "
                    f"(epoch {best_epoch})\n"
                )
                break

    # ------------------------------------------------------------------
    # 6. Save Encoder + Scaler
    # ------------------------------------------------------------------

    # Restore best encoder weights (early stopping) and save them
    if best_encoder_state is not None:
        model.encoder.load_state_dict(best_encoder_state)

    torch.save(model.encoder.state_dict(), MODEL_SAVE_PATH)


    import joblib
    joblib.dump(train_dataset.scaler, SCALER_SAVE_PATH)

    print("\n====================================")
    print(" TRAINING COMPLETE")
    print("====================================")
    print(f"Encoder saved to: {MODEL_SAVE_PATH}")
    print(f"Scaler saved to:  {SCALER_SAVE_PATH}\n")


# =====================================================================================
# ENTRY POINT
# =====================================================================================

if __name__ == "__main__":
    train()
